---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

{% if author.googlescholar %}
  You can also find my articles on <u><a href="{{author.googlescholar}}">my Google Scholar profile</a>.</u>
{% endif %}

{% include base_path %}

<p>I published a series of papers during my FAIR residency examining the causal role of selectivity and the state of interpretability research in deep neural networks. The <u><a href="https://openreview.net/forum?id=8nl0k08uMi">first paper</a></u>, accepted to <i>ICLR</i> 2021, shows that easily-interpretable neurons in deep neural networks (DNNs) may actually impair DNN function. The <u><a href="https://arxiv.org/abs/2010.07693">second paper</a></u>, presented at the 2020 <i>ICML Workshop on Uncertainty and Robustness in Deep Learning</i>, examines the effects of easily-interpretable neurons on DNN robustness to adversarial and naturalistic perturbations. The <u><a href="https://ml-retrospectives.github.io/neurips2020/camera_ready/4.pdf">final paper</a></u>, presented at <i>ML Retrospectives, Surveys & Meta-Analyses @NeurIPS 2020</i>, makes a plea for greater rigor in interpretability research. See <u><a href="https://ai.facebook.com/blog/easy-to-interpret-neurons-may-hinder-learning-in-deep-neural-networks/">this blog post</a></u> for an accessible summary of all three papers.</p>

<p>See <u><a href="/files/matthew_leavitt_cv.pdf">my CV</a></u> for a complete list of publications, talks,
  posters, awards, and professional + biographical ephemeralia.</p>

<p>See <u><a href="/files/leavitt_ethesis.pdf"> my thesis</a></u> for
  nearly 300 pages of light reading.</p>

{% for post in site.publications reversed %}
  {% include archive-single.html %}
{% endfor %}
